{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"data/train/images/246-ktp_jpg.rf.fa9bf940931b0387b060f828f5ba6f8e.jpg\"\n",
    "\n",
    "# Convert image to RGB\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from doctr.models import ocr_predictor\n",
    "# from doctr.io import DocumentFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ocr_predictor(det_arch='fast_small',reco_arch='crnn_mobilenet_v3_small',pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_img_doc = DocumentFile.from_images(image_path)\n",
    "# result = model(single_img_doc)\n",
    "# export = result.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export['pages'][0][\"blocks\"][0][\"lines\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "\n",
    "reader = easyocr.Reader(['id'])\n",
    "export = reader.readtext(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[np.int32(176), np.int32(73)],\n",
       "   [np.int32(482), np.int32(73)],\n",
       "   [np.int32(482), np.int32(106)],\n",
       "   [np.int32(176), np.int32(106)]],\n",
       "  'PROVINSI SULAWESI SELATAN',\n",
       "  np.float64(0.9580166371000649)),\n",
       " ([[np.int32(223), np.int32(99)],\n",
       "   [np.int32(433), np.int32(99)],\n",
       "   [np.int32(433), np.int32(135)],\n",
       "   [np.int32(223), np.int32(135)]],\n",
       "  'KABUPATEN MAROS',\n",
       "  np.float64(0.9961174373559115)),\n",
       " ([[np.int32(34), np.int32(140)],\n",
       "   [np.int32(88), np.int32(140)],\n",
       "   [np.int32(88), np.int32(172)],\n",
       "   [np.int32(34), np.int32(172)]],\n",
       "  'NIK',\n",
       "  np.float64(0.9756025764265064)),\n",
       " ([[np.int32(164), np.int32(137)],\n",
       "   [np.int32(433), np.int32(137)],\n",
       "   [np.int32(433), np.int32(178)],\n",
       "   [np.int32(164), np.int32(178)]],\n",
       "  '7309012705930004',\n",
       "  np.float64(0.9896844845657495)),\n",
       " ([[np.int32(33), np.int32(187)],\n",
       "   [np.int32(86), np.int32(187)],\n",
       "   [np.int32(86), np.int32(214)],\n",
       "   [np.int32(33), np.int32(214)]],\n",
       "  'Nama',\n",
       "  np.float64(0.959918737411499)),\n",
       " ([[np.int32(178), np.int32(188)],\n",
       "   [np.int32(370), np.int32(188)],\n",
       "   [np.int32(370), np.int32(216)],\n",
       "   [np.int32(178), np.int32(216)]],\n",
       "  'MUH DHIAUR RAHMAN',\n",
       "  np.float64(0.9242586135968651)),\n",
       " ([[np.int32(34), np.int32(212)],\n",
       "   [np.int32(168), np.int32(212)],\n",
       "   [np.int32(168), np.int32(238)],\n",
       "   [np.int32(34), np.int32(238)]],\n",
       "  'TempadIgl Lahır',\n",
       "  np.float64(0.607199533286732)),\n",
       " ([[np.int32(178), np.int32(212)],\n",
       "   [np.int32(342), np.int32(212)],\n",
       "   [np.int32(342), np.int32(236)],\n",
       "   [np.int32(178), np.int32(236)]],\n",
       "  'MAROS; 27-05-1993',\n",
       "  np.float64(0.4445627033680521)),\n",
       " ([[np.int32(35), np.int32(237)],\n",
       "   [np.int32(141), np.int32(237)],\n",
       "   [np.int32(141), np.int32(257)],\n",
       "   [np.int32(35), np.int32(257)]],\n",
       "  'Jenis Kelamin',\n",
       "  np.float64(0.5620296440559994)),\n",
       " ([[np.int32(178), np.int32(236)],\n",
       "   [np.int32(264), np.int32(236)],\n",
       "   [np.int32(264), np.int32(260)],\n",
       "   [np.int32(178), np.int32(260)]],\n",
       "  'LAKI LAKI',\n",
       "  np.float64(0.9635035658045185)),\n",
       " ([[np.int32(314), np.int32(236)],\n",
       "   [np.int32(424), np.int32(236)],\n",
       "   [np.int32(424), np.int32(262)],\n",
       "   [np.int32(314), np.int32(262)]],\n",
       "  'Gol Darah :B',\n",
       "  np.float64(0.2644904815608581)),\n",
       " ([[np.int32(31), np.int32(256)],\n",
       "   [np.int32(94), np.int32(256)],\n",
       "   [np.int32(94), np.int32(283)],\n",
       "   [np.int32(31), np.int32(283)]],\n",
       "  'Alamat',\n",
       "  np.float64(0.9957916225017636)),\n",
       " ([[np.int32(178), np.int32(257)],\n",
       "   [np.int32(366), np.int32(257)],\n",
       "   [np.int32(366), np.int32(285)],\n",
       "   [np.int32(178), np.int32(285)]],\n",
       "  'PP DARUL ISTIQAMAH',\n",
       "  np.float64(0.9110322801690217)),\n",
       " ([[np.int32(66), np.int32(280)],\n",
       "   [np.int32(132), np.int32(280)],\n",
       "   [np.int32(132), np.int32(304)],\n",
       "   [np.int32(66), np.int32(304)]],\n",
       "  'RIIRW',\n",
       "  np.float64(0.9116773596917991)),\n",
       " ([[np.int32(179), np.int32(283)],\n",
       "   [np.int32(243), np.int32(283)],\n",
       "   [np.int32(243), np.int32(303)],\n",
       "   [np.int32(179), np.int32(303)]],\n",
       "  '001/002',\n",
       "  np.float64(0.9991834166765811)),\n",
       " ([[np.int32(66), np.int32(304)],\n",
       "   [np.int32(144), np.int32(304)],\n",
       "   [np.int32(144), np.int32(328)],\n",
       "   [np.int32(66), np.int32(328)]],\n",
       "  'KellDesa',\n",
       "  np.float64(0.8776637029208665)),\n",
       " ([[np.int32(178), np.int32(304)],\n",
       "   [np.int32(254), np.int32(304)],\n",
       "   [np.int32(254), np.int32(328)],\n",
       "   [np.int32(178), np.int32(328)]],\n",
       "  'BONTOA',\n",
       "  np.float64(0.9610585090054193)),\n",
       " ([[np.int32(69), np.int32(329)],\n",
       "   [np.int32(157), np.int32(329)],\n",
       "   [np.int32(157), np.int32(349)],\n",
       "   [np.int32(69), np.int32(349)]],\n",
       "  'Kecamatan',\n",
       "  np.float64(0.42379222268208905)),\n",
       " ([[np.int32(178), np.int32(326)],\n",
       "   [np.int32(250), np.int32(326)],\n",
       "   [np.int32(250), np.int32(352)],\n",
       "   [np.int32(178), np.int32(352)]],\n",
       "  'MANDAI',\n",
       "  np.float64(0.979870872208396)),\n",
       " ([[np.int32(32), np.int32(350)],\n",
       "   [np.int32(92), np.int32(350)],\n",
       "   [np.int32(92), np.int32(376)],\n",
       "   [np.int32(32), np.int32(376)]],\n",
       "  'Agama',\n",
       "  np.float64(0.8863822843073338)),\n",
       " ([[np.int32(176), np.int32(350)],\n",
       "   [np.int32(236), np.int32(350)],\n",
       "   [np.int32(236), np.int32(374)],\n",
       "   [np.int32(176), np.int32(374)]],\n",
       "  'ISUAM',\n",
       "  np.float64(0.5315821869926167)),\n",
       " ([[np.int32(32), np.int32(372)],\n",
       "   [np.int32(302), np.int32(372)],\n",
       "   [np.int32(302), np.int32(398)],\n",
       "   [np.int32(32), np.int32(398)]],\n",
       "  'Status Perkewınan BELUM KAWIN',\n",
       "  np.float64(0.8839187916491348)),\n",
       " ([[np.int32(28), np.int32(393)],\n",
       "   [np.int32(115), np.int32(393)],\n",
       "   [np.int32(115), np.int32(425)],\n",
       "   [np.int32(28), np.int32(425)]],\n",
       "  'Pekerjaan',\n",
       "  np.float64(0.986444325444512)),\n",
       " ([[np.int32(176), np.int32(396)],\n",
       "   [np.int32(362), np.int32(396)],\n",
       "   [np.int32(362), np.int32(422)],\n",
       "   [np.int32(176), np.int32(422)]],\n",
       "  'PELAJARIMAHASISWA',\n",
       "  np.float64(0.9939336882566581)),\n",
       " ([[np.int32(499), np.int32(409)],\n",
       "   [np.int32(555), np.int32(409)],\n",
       "   [np.int32(555), np.int32(429)],\n",
       "   [np.int32(499), np.int32(429)]],\n",
       "  'MAROS',\n",
       "  np.float64(0.8002495326342579)),\n",
       " ([[np.int32(29), np.int32(419)],\n",
       "   [np.int32(218), np.int32(419)],\n",
       "   [np.int32(218), np.int32(448)],\n",
       "   [np.int32(29), np.int32(448)]],\n",
       "  'Kewargancgaraan : WNI',\n",
       "  np.float64(0.6598741772607664)),\n",
       " ([[np.int32(487), np.int32(424)],\n",
       "   [np.int32(566), np.int32(424)],\n",
       "   [np.int32(566), np.int32(450)],\n",
       "   [np.int32(487), np.int32(450)]],\n",
       "  '1710 2017',\n",
       "  np.float64(0.689881044640665)),\n",
       " ([[np.int32(30), np.int32(442)],\n",
       "   [np.int32(96), np.int32(442)],\n",
       "   [np.int32(96), np.int32(468)],\n",
       "   [np.int32(30), np.int32(468)]],\n",
       "  'Berlaku',\n",
       "  np.float64(0.6967363314109271)),\n",
       " ([[np.int32(176), np.int32(442)],\n",
       "   [np.int32(314), np.int32(442)],\n",
       "   [np.int32(314), np.int32(468)],\n",
       "   [np.int32(176), np.int32(468)]],\n",
       "  'SEUMUR HIDUP',\n",
       "  np.float64(0.9988502676506469)),\n",
       " ([[np.float64(92.10517502059216), np.float64(437.1365600658949)],\n",
       "   [np.float64(154.92856118055187), np.float64(448.34920862654405)],\n",
       "   [np.float64(148.89482497940784), np.float64(474.8634399341051)],\n",
       "   [np.float64(86.07143881944815), np.float64(463.65079137345595)]],\n",
       "  'Hingge',\n",
       "  np.float64(0.588763599580401))]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_objectness_scores(blocks):\n",
    "#     scores = []\n",
    "#     for block in blocks:\n",
    "#         scores.append(block.get('objectness_score', 0))\n",
    "#         if 'lines' in block:\n",
    "#             for line in block['lines']:\n",
    "#                 scores.append(line.get('objectness_score', 0))\n",
    "#                 if 'words' in line:\n",
    "#                     for word in line['words']:\n",
    "#                         scores.append(word.get('objectness_score', 0))\n",
    "#         if 'words' in block:\n",
    "#             for word in block['words']:\n",
    "#                 scores.append(word.get('objectness_score', 0))\n",
    "#     return scores\n",
    "\n",
    "# # Extract scores\n",
    "# scores = extract_objectness_scores(export['pages'][0]['blocks'])\n",
    "\n",
    "# # Calculate average\n",
    "# average_score = sum(scores) / len(scores) if scores else 0\n",
    "# average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRTextProcessor:\n",
    "    def __init__(self, tolerance=15):\n",
    "        self.tolerance = tolerance\n",
    "    \n",
    "    def process_ocr_result(self, result):\n",
    "        \"\"\"Process OCR result and return corrected text alignments.\"\"\"\n",
    "        # Extract all words with their coordinates and text\n",
    "        words = []\n",
    "        # for page in result['pages']:\n",
    "        #     for block in page['blocks']:\n",
    "        #         for line in block.get('lines', []):\n",
    "        #             for word in line.get('words', []):\n",
    "        #                 # Get coordinates from geometry\n",
    "        #                 coords = word['geometry']\n",
    "        #                 # Convert coordinates to more readable format\n",
    "        #                 x1, y1 = coords[0]\n",
    "        #                 x2, y2 = coords[1]\n",
    "                        \n",
    "        #                 words.append({\n",
    "        #                     'text': word['value'],\n",
    "        #                     'coords': {\n",
    "        #                         'y1': float(y1),\n",
    "        #                         'y2': float(y2),\n",
    "        #                         'x1': float(x1),\n",
    "        #                         'x2': float(x2)\n",
    "        #                     }\n",
    "        #                 })\n",
    "        for item in result:\n",
    "            coords_raw, text, confidence = item\n",
    "            x1, y1 = coords_raw[0]  # Top-left\n",
    "            x2, y2 = coords_raw[2]  # Bottom-right\n",
    "            words.append({\n",
    "                'text': text,\n",
    "                'coords': {\n",
    "                    'y1': float(y1),\n",
    "                    'y2': float(y2),\n",
    "                    'x1': float(x1),\n",
    "                    'x2': float(x2)\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Sort words by y-coordinate first\n",
    "        words.sort(key=lambda x: x['coords']['y1'])\n",
    "        \n",
    "        # Group words that are on the same line\n",
    "        lines = []\n",
    "        current_line = [words[0]] if words else []\n",
    "        \n",
    "        for word in words[1:]:\n",
    "            last_word = current_line[-1] if current_line else None\n",
    "            \n",
    "            if last_word and self._is_same_line(last_word['coords'], word['coords']):\n",
    "                current_line.append(word)\n",
    "            else:\n",
    "                if current_line:\n",
    "                    # Sort words in the line by x-coordinate before adding to lines\n",
    "                    current_line.sort(key=lambda x: x['coords']['x1'])\n",
    "                    lines.append(current_line)\n",
    "                current_line = [word]\n",
    "        \n",
    "        if current_line:\n",
    "            # Sort the last line by x-coordinate\n",
    "            current_line.sort(key=lambda x: x['coords']['x1'])\n",
    "            lines.append(current_line)\n",
    "        \n",
    "        # Convert grouped words to text\n",
    "        formatted_text = []\n",
    "        for line in lines:\n",
    "            line_text = ' '.join(word['text'] for word in line)\n",
    "            formatted_text.append(line_text)\n",
    "        \n",
    "        return formatted_text\n",
    "    \n",
    "    def _is_same_line(self, coords1, coords2, tolerance_factor=0.5):\n",
    "        \"\"\"\n",
    "        Check if two words are on the same line based on vertical coordinates\n",
    "        Using a tolerance factor relative to text height\n",
    "        \"\"\"\n",
    "        height1 = coords1['y2'] - coords1['y1']\n",
    "        height2 = coords2['y2'] - coords2['y1']\n",
    "        avg_height = (height1 + height2) / 2\n",
    "        \n",
    "        mid1 = (coords1['y1'] + coords1['y2']) / 2\n",
    "        mid2 = (coords2['y1'] + coords2['y2']) / 2\n",
    "        \n",
    "        return abs(mid1 - mid2) < (avg_height * tolerance_factor)\n",
    "\n",
    "# Example usage\n",
    "def process_id_card(ocr_result):\n",
    "    processor = OCRTextProcessor()\n",
    "    formatted_text = processor.process_ocr_result(ocr_result)\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEntityExtractor:\n",
    "    def __init__(self):\n",
    "        # Define fields with their keywords and tolerance levels\n",
    "        self.fields = [\n",
    "            {'name': 'provinsi', 'keywords': ['provinsi'], 'tolerance': 2},\n",
    "            {'name': 'kabupaten', 'keywords': ['kabupaten', 'kota'], 'tolerance': 2},\n",
    "            {'name': 'nik', 'keywords': ['nik'], 'tolerance': 1},\n",
    "            {'name': 'nama', 'keywords': ['nama'], 'tolerance': 1},\n",
    "            {'name': 'tempat_tgl_lahir', 'keywords': ['tempat/tgl', 'tempat/tgilahir', 'tempat','tompat/tgllah'], 'tolerance': 3},\n",
    "            # {'name': 'tanggal_lahir', 'keywords': ['tgl', 'tanggal'], 'tolerance': 2},\n",
    "            {'name': 'jenis_kelamin', 'keywords': ['jenis kelamin', 'kelamin'], 'tolerance': 2},\n",
    "            {'name': 'alamat', 'keywords': ['alamat'], 'tolerance': 2},\n",
    "            {'name': 'rt_rw', 'keywords': ['rt/rw', 'rtrw'], 'tolerance': 2},\n",
    "            {'name': 'kel_desa', 'keywords': ['kel/desa', 'kelurahan', 'desa'], 'tolerance': 2},\n",
    "            {'name': 'kecamatan', 'keywords': ['kecamatan', 'kec'], 'tolerance': 3},\n",
    "            {'name': 'agama', 'keywords': ['agama'], 'tolerance': 2},\n",
    "            {'name': 'status_perkawinan', 'keywords': ['status perkawinan', 'perkawinan'], 'tolerance': 3},\n",
    "            {'name': 'pekerjaan', 'keywords': ['pekerjaan', 'kerja'], 'tolerance': 3},\n",
    "            {'name': 'kewarganegaraan', 'keywords': ['kewarganegaraan'], 'tolerance': 4},\n",
    "            {'name': 'berlaku_hingga', 'keywords': ['berlaku hingga', 'hingga'], 'tolerance': 3}\n",
    "        ]\n",
    "\n",
    "    def levenshtein_distance(self, s1, s2):\n",
    "        \"\"\"Calculate the Levenshtein distance between two strings\"\"\"\n",
    "        if len(s1) < len(s2):\n",
    "            return self.levenshtein_distance(s2, s1)\n",
    "\n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "\n",
    "        previous_row = range(len(s2) + 1)\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + (c1 != c2)\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "            previous_row = current_row\n",
    "\n",
    "        return previous_row[-1]\n",
    "\n",
    "    def find_field_match(self, line):\n",
    "        \"\"\"Find matching field for a line based on Levenshtein distance\"\"\"\n",
    "        words = line.lower().split()\n",
    "        if not words:\n",
    "            return None\n",
    "\n",
    "        best_match = None\n",
    "        min_distance = float('inf')\n",
    "        \n",
    "        for field in self.fields:\n",
    "            for keyword in field['keywords']:\n",
    "                keyword_parts = keyword.lower().split()\n",
    "                \n",
    "                # Try matching with first word(s) of line\n",
    "                for i in range(min(len(words), len(keyword_parts) + 1)):\n",
    "                    line_part = ' '.join(words[:i+1])\n",
    "                    distance = self.levenshtein_distance(line_part, keyword)\n",
    "                    \n",
    "                    if distance < min_distance and distance <= field['tolerance']:\n",
    "                        min_distance = distance\n",
    "                        best_match = field\n",
    "\n",
    "        return best_match\n",
    "\n",
    "    def extract_value(self, line, field):\n",
    "        \"\"\"Extract value from a line based on field type\"\"\"\n",
    "        # Split line into parts\n",
    "        parts = line.split()\n",
    "        \n",
    "        # Find where the field name ends\n",
    "        field_end = 0\n",
    "        for i, part in enumerate(parts):\n",
    "            for keyword in field['keywords']:\n",
    "                if self.levenshtein_distance(part.lower(), keyword.lower()) <= field['tolerance']:\n",
    "                    field_end = i + 1\n",
    "                    break\n",
    "        \n",
    "        # Extract value portion\n",
    "        value = ' '.join(parts[field_end:]).strip()\n",
    "        \n",
    "        # Clean up common artifacts\n",
    "        value = value.replace(':', '').strip()\n",
    "        \n",
    "        return value if value else None\n",
    "\n",
    "    def extract_entities(self, lines):\n",
    "        \"\"\"Extract entities from list of lines\"\"\"\n",
    "        entities = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # Find matching field\n",
    "            field = self.find_field_match(line)\n",
    "            if field:\n",
    "                value = self.extract_value(line, field)\n",
    "                if value:\n",
    "                    # Special handling for fields that might have multiple parts\n",
    "                    if field['name'] in entities:\n",
    "                        if isinstance(entities[field['name']], list):\n",
    "                            entities[field['name']].append(value)\n",
    "                        else:\n",
    "                            entities[field['name']] = [entities[field['name']], value]\n",
    "                    else:\n",
    "                        entities[field['name']] = value\n",
    "\n",
    "        return entities\n",
    "\n",
    "# Example usage\n",
    "def extract_id_card_info(lines):\n",
    "    extractor = TextEntityExtractor()\n",
    "    return extractor.extract_entities(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROVINSI SULAWESI SELATAN',\n",
       " 'KABUPATEN MAROS',\n",
       " 'NIK 7309012705930004',\n",
       " 'NAMA MUH DHIAUR RAHMAN',\n",
       " 'TEMPADIGL LAH MAROS 27 05 1993',\n",
       " 'JENIS KELAMIN LAKI LAKI GOL DARAH',\n",
       " 'ALAMAT PP DARUL ISTIQAMAH',\n",
       " 'RIIRW 001 002',\n",
       " 'KELLDESA BONTOA',\n",
       " 'KECAMATAN MANDAI',\n",
       " 'AGAMA ISUAM',\n",
       " 'STATUS PERKEW NAN BELUM KAWIN',\n",
       " 'PEKERJAAN PELAJARIMAHASISWA MAROS',\n",
       " 'KEWARGANCGARAAN WNI 1710 2017',\n",
       " 'BERLAKU HINGGE SEUMUR HIDUP']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the processor\n",
    "processor = OCRTextProcessor()\n",
    "formatted_text = processor.process_ocr_result(export)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters except hyphen (-)\n",
    "    text = re.sub(r'[^A-Za-z0-9]', ' ', text)\n",
    "    # Convert to uppercase\n",
    "    text = text.upper()\n",
    "    # Remove single characters\n",
    "    text = ' '.join(word for word in text.split() if len(word) > 1)\n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to each string in the list\n",
    "processed_data = [preprocess_text(item) for item in formatted_text]\n",
    "processed_data = [item for item in processed_data if item]\n",
    "\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'provinsi': 'SULAWESI SELATAN',\n",
       " 'kabupaten': 'MAROS',\n",
       " 'nik': '7309012705930004',\n",
       " 'nama': 'MUH DHIAUR RAHMAN',\n",
       " 'tempat_tgl_lahir': 'LAH MAROS 27 05 1993',\n",
       " 'jenis_kelamin': 'LAKI-LAKI',\n",
       " 'alamat': 'PP DARUL ISTIQAMAH',\n",
       " 'rt_rw': '001 002',\n",
       " 'kel_desa': 'BONTOA',\n",
       " 'kecamatan': 'MANDAI',\n",
       " 'agama': 'ISLAM',\n",
       " 'status_perkawinan': 'BELUM KAWIN',\n",
       " 'pekerjaan': 'PELAJARIMAHASISWA MAROS',\n",
       " 'kewarganegaraan': 'WNI 1710 2017',\n",
       " 'berlaku_hingga': 'SEUMUR HIDUP',\n",
       " 'tanggal_lahir': '27-05-1993',\n",
       " 'tempat_lahir': 'LAH MAROS'}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = extract_id_card_info(processed_data)\n",
    "from datetime import datetime\n",
    "def post_processing(data):\n",
    "    import re\n",
    "\n",
    "    # Helper function to count matching characters\n",
    "    def count_matching_chars(a, char):\n",
    "        count = 0\n",
    "        for c in char:\n",
    "            if c in a:\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    def extract_date_and_place(tempat_tgl_lahir):\n",
    "        # Regular expression to find 'DD MM YYYY' in the text\n",
    "        date_match = re.search(r'\\b\\d{2}\\s\\d{2}\\s\\d{4}\\b', tempat_tgl_lahir)\n",
    "        if date_match:\n",
    "            # Extract the date\n",
    "            date = date_match.group()\n",
    "            day, month, year = date.split()\n",
    "            formatted_date = f\"{day}-{month}-{year}\"\n",
    "            # Extract the place (everything before the date)\n",
    "            place = tempat_tgl_lahir[:date_match.start()].strip()\n",
    "        else:\n",
    "            # If no date, use current date and entire input as place\n",
    "            formatted_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "            place = tempat_tgl_lahir.strip()\n",
    "        \n",
    "        return formatted_date, place\n",
    "\n",
    "    def correct_agama(agama):\n",
    "        target_words = [\"ISLAM\", \"KRISTEN\", \"KATOLIK\", \"HINDU\", \"BUDDHA\", \"KONGHUCU\"]\n",
    "        agama = agama.lower()\n",
    "        match_scores = {word: count_matching_chars(agama, word.lower()) for word in target_words}\n",
    "        most_likely_agama = max(match_scores, key=match_scores.get)\n",
    "        return most_likely_agama\n",
    "    \n",
    "    def correct_jenis_kelamin(jenis_kelamin):\n",
    "        target_words = [\"LAKI-LAKI\", \"PEREMPUAN\"]\n",
    "        jenis_kelamin = jenis_kelamin.lower()\n",
    "        match_scores = {word: count_matching_chars(jenis_kelamin, word.lower()) for word in target_words}\n",
    "        most_likely_jenis_kelamin = max(match_scores, key=match_scores.get)\n",
    "        return most_likely_jenis_kelamin\n",
    "    \n",
    "    def correct_status_perkawinan(status_perkawinan):\n",
    "        target_words = [\"KAWIN\", \"BELUM KAWIN\", \"CERAI HIDUP\", \"CERAI MATI\"]\n",
    "        status_perkawinan = status_perkawinan.lower()\n",
    "        match_scores = {word: count_matching_chars(status_perkawinan, word.lower()) for word in target_words}\n",
    "        most_likely_status_perkawinan = max(match_scores, key=match_scores.get)\n",
    "        return most_likely_status_perkawinan\n",
    "    \n",
    "    def correct_pekerjaan(pekerjaan):\n",
    "        target_words = [\n",
    "            \"BELUM/TIDAK BEKERJA\", \"MENGURUS RUMAH TANGGA\", \"PELAJAR/MAHASISWA\", \"PENSIUNAN\",\n",
    "            \"PEGAWAI NEGERI SIPIL\", \"TENTARA NASIONAL INDONESIA\", \"KEPOLISIAN RI\", \"PERDAGANGAN\",\n",
    "            \"PETANI/PEKEBUN\", \"PETERNAK\", \"NELAYAN/PERIKANAN\", \"INDUSTRI\", \"KONSTRUKSI\", \"TRANSPORTASI\",\n",
    "            \"KARYAWAN SWASTA\", \"KARYAWAN BUMN\", \"KARYAWAN BUMD\", \"KARYAWAN HONORER\", \"BURUH HARIAN LEPAS\",\n",
    "            \"BURUH TANI/PERKEBUNAN\", \"BURUH NELAYAN/PERIKANAN\", \"BURUH PETERNAKAN\", \"PEMBANTU RUMAH TANGGA\",\n",
    "            \"TUKANG CUKUR\", \"TUKANG LISTRIK\", \"TUKANG BATU\", \"TUKANG KAYU\", \"TUKANG SOL SEPATU\",\n",
    "            \"TUKANG LAS/PANDAI BESI\", \"TUKANG JAHIT\", \"TUKANG GIGI\", \"PENATA RIAS\", \"PENATA BUSANA\",\n",
    "            \"PENATA RAMBUT\", \"MEKANIK\", \"SENIMAN\", \"TABIB\", \"PARAJI\", \"PERANCANG BUSANA\", \"PENTERJEMAH\",\n",
    "            \"IMAM MASJID\", \"PENDETA\", \"PASTOR\", \"WARTAWAN\", \"USTADZ/MUBALIGH\", \"JURU MASAK\", \"PROMOTOR ACARA\",\n",
    "            \"ANGGOTA DPR-RI\", \"ANGGOTA DPD\", \"ANGGOTA BPK\", \"PRESIDEN\", \"WAKIL PRESIDEN\",\n",
    "            \"ANGGOTA MAHKAMAH KONSTITUSI\", \"ANGGOTA KABINET/KEMENTERIAN\", \"DUTA BESAR\", \"GUBERNUR\",\n",
    "            \"WAKIL GUBERNUR\", \"BUPATI\", \"WAKIL BUPATI\", \"WALIKOTA\", \"WAKIL WALIKOTA\", \"ANGGOTA DPRD PROVINSI\",\n",
    "            \"ANGGOTA DPRD KABUPATEN/KOTA\", \"DOSEN\", \"GURU\", \"PILOT\", \"PENGACARA\", \"NOTARIS\", \"ARSITEK\",\n",
    "            \"AKUNTAN\", \"KONSULTAN\", \"DOKTER\", \"BIDAN\", \"PERAWAT\", \"APOTEKER\", \"PSIKIATER/PSIKOLOG\",\n",
    "            \"PENYIAR TELEVISI\", \"PENYIAR RADIO\", \"PELAUT\", \"PENELITI\", \"SOPIR\", \"PIALANG\", \"PARANORMAL\",\n",
    "            \"PEDAGANG\", \"PERANGKAT DESA\", \"KEPALA DESA\", \"BIARAWATI\", \"WIRASWASTA\"\n",
    "        ]\n",
    "        pekerjaan = pekerjaan.upper()\n",
    "        match_scores = {word: count_matching_chars(pekerjaan, word) for word in target_words}\n",
    "        most_likely_pekerjaan = max(match_scores, key=match_scores.get)\n",
    "        return most_likely_pekerjaan\n",
    "    \n",
    "    if 'jenis_kelamin' in data:\n",
    "        data['jenis_kelamin'] = correct_jenis_kelamin(data['jenis_kelamin'])\n",
    "    if 'agama' in data:\n",
    "        data['agama'] = correct_agama(data['agama'])\n",
    "    # if 'pekerjaan' in data:\n",
    "    #     data['pekerjaan'] = correct_pekerjaan(data['pekerjaan'])\n",
    "    if 'status_perkawinan' in data:\n",
    "        data['status_perkawinan'] = correct_status_perkawinan(data['status_perkawinan'])\n",
    "    if 'tempat_tgl_lahir' in data:\n",
    "        extracted_date, extracted_place = extract_date_and_place(data['tempat_tgl_lahir'])\n",
    "        data['tanggal_lahir'] = extracted_date\n",
    "        data['tempat_lahir'] = extracted_place\n",
    "    return data\n",
    "\n",
    "result = post_processing(entities)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"widi\"\n",
    "char = \"wd\"\n",
    "\n",
    "# count how many same characters founded are in the string\n",
    "count = 0\n",
    "for i in range(len(char)):\n",
    "    if char[i] in a:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted date: 27 05 1993\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'provinsi': 'SULAWESI SELATAN',\n",
    "    'kabupaten': 'MAROS',\n",
    "    'nik': '7309012705930004',\n",
    "    'nama': 'MUH DHIAUR RAHMAN',\n",
    "    'tempat_tgl_lahir': 'LAH MAROS 27 05 1993',\n",
    "    'jenis_kelamin': 'LAKI-LAKI',\n",
    "    'alamat': 'PP DARUL ISTIQAMAH',\n",
    "    'rt_rw': '001 002',\n",
    "    'kel_desa': 'BONTOA',\n",
    "    'kecamatan': 'MANDAI',\n",
    "    'agama': 'ISLAM',\n",
    "    'status_perkawinan': 'BELUM KAWIN',\n",
    "    'pekerjaan': 'TENTARA NASIONAL INDONESIA',\n",
    "    'kewarganegaraan': 'WNI 1710 2017',\n",
    "    'berlaku_hingga': 'SEUMUR HIDUP'\n",
    "}\n",
    "\n",
    "# Extract 'tempat_tgl_lahir'\n",
    "tempat_tgl_lahir = data['tempat_tgl_lahir']\n",
    "\n",
    "# Regular expression to extract the date in 'DD MM YYYY' format\n",
    "date_match = re.search(r'\\b\\d{2}\\s\\d{2}\\s\\d{4}\\b', tempat_tgl_lahir)\n",
    "\n",
    "# Extract and display the date\n",
    "if date_match:\n",
    "    date = date_match.group()\n",
    "    print(\"Extracted date:\", date)\n",
    "else:\n",
    "    print(\"Date not found in 'tempat_tgl_lahir'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the result:\n",
    "# provinsi: JAWA TENGAH kabupaten: . PURBALINGGA nik: - 330315040458000 nama: IFTAH SADJAD AHMADI tempat_lahir: PURBAUNGGA tanggal_lahir: 04-06-1998 jenis_kelamin: LAKRAKE Gol Darah a alamat: PERLMEDMIPAOAMMAGNFS rt_rw: 001003 kel_desa: BOJANEGARA kecamatan: PADAMARA agama: ISLAM status_perkawinan: BELUMKAWIN PUHBALINGGA pekerjaan: PELLARMAHASISHA 19092015 kewarganegaraan: WNI berlaku_hingga: SEUMUR RIDUP\n",
    "\n",
    "# do validation like:\n",
    "# 1. there is no number in \"pekerjaan\", so if there is detected number, remove the number\n",
    "# 2. in status perkawinan mostly the value is : belum kawin, kawin, cerai mati, cerai hidup. check if tolerance in that value. in my case it detected BELUMKAWIN PUHBALINGGA. remove else the value i gave you earlier (PUHBALINGGA)\n",
    "# 3. Gol darah is completly new key"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ck-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
